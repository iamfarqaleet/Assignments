{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Anomaly Detection & Time Series | Assignment DA-AG-018"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 1: What is Anomaly Detection? Explain its types (point, contextual, and collective anomalies) with examples.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "**Anomaly Detection** (also known as outlier detection) is the process of identifying data points, events, or observations that deviate significantly from the majority of the data and do not conform to an expected pattern. These non-conforming instances are referred to as anomalies, outliers, exceptions, or novelties.\n",
                "\n",
                "Anomalies can be broadly categorized into three types:\n",
                "\n",
                "#### 1. Point Anomalies\n",
                "A point anomaly is a single instance of data that is anomalous with respect to the rest of the data. It is the simplest type of anomaly and the primary focus of most research.\n",
                "- **Example: Credit Card Fraud** üí≥\n",
                "  A user typically makes purchases under \\$100 in their home city. A new transaction of \\$5,000 from a different country would be a point anomaly and a strong indicator of fraud.\n",
                "\n",
                "#### 2. Contextual Anomalies (Conditional Anomalies)\n",
                "A contextual anomaly is a data instance that is considered anomalous within a specific context, but not otherwise. The algorithm must consider contextual information (e.g., time, location) to identify it.\n",
                "- **Example: Seasonal Sales** üß•\n",
                "  A high volume of winter coat sales is normal in December (context: winter). However, the exact same sales volume in July (context: summer) would be a contextual anomaly, perhaps indicating a data error or a highly unusual marketing success.\n",
                "\n",
                "#### 3. Collective Anomalies\n",
                "A collective anomaly is a collection of related data instances that is anomalous with respect to the entire dataset. The individual data points within the collection may not be anomalies by themselves, but their occurrence together as a collection is.\n",
                "- **Example: Electrocardiogram (ECG)** ‚ù§Ô∏è\n",
                "  In an ECG reading that monitors a human heartbeat, a single beat might be within the normal range. However, a prolonged period of a flat signal (a collection of low-value points), while individually normal, collectively indicates a serious anomaly like cardiac arrest."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 2: Compare Isolation Forest, DBSCAN, and Local Outlier Factor in terms of their approach and suitable use cases.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "| Feature         | Isolation Forest                                                                                                       | DBSCAN (Density-Based Spatial Clustering)                                                              | Local Outlier Factor (LOF)                                                                                             |\n",
                "| :-------------- | :--------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------- |\n",
                "| **Approach** | **Isolation-based**. It builds an ensemble of decision trees. Anomalies are easier to \"isolate\" (require fewer splits to separate from other points) and will thus have shorter average path lengths in the trees. | **Density-based**. It groups together points that are closely packed. Points in low-density regions that don't belong to any cluster are identified as noise/anomalies. | **Local Density-based**. It measures the local density deviation of a data point with respect to its neighbors. An object is an outlier if its local density is significantly lower than that of its neighbors. |\n",
                "| **Key Idea** | Anomalies are \"few and different.\"                                                                                   | Anomalies are in sparse regions.                                                                       | Anomalies are in regions of lower local density compared to their surroundings.                                        |\n",
                "| **Parameters** | `n_estimators`, `contamination` (expected proportion of outliers).                                                     | `eps` (neighborhood radius), `min_samples` (minimum points in a neighborhood).                         | `n_neighbors` (number of neighbors to consider).                                                                       |\n",
                "| **Strengths** | - Very fast and efficient, especially for large datasets.<br>- Works well in high-dimensional spaces.<br>- Requires few parameters. | - Can find arbitrarily shaped clusters and anomalies.<br>- Does not assume a specific distribution.<br>- Robust to noise. | - Effective at detecting anomalies in datasets with varying densities.<br>- Does not require a global density parameter. |\n",
                "| **Weaknesses** | - May struggle if anomalies are clustered together.<br>- Performance can degrade if normal and anomalous data are not clearly separable. | - Struggles with datasets of varying densities.<br>- Sensitive to the choice of `eps` and `min_samples`.<br>- Can be slow on large datasets. | - Computationally expensive ($O(n^2)$), not suitable for very large datasets.<br>- Can be sensitive to the `n_neighbors` parameter. |\n",
                "| **Use Case** | **Fraud detection in finance or network security**, where fast processing of large, high-dimensional datasets is required. | **Geospatial analysis** to find noisy GPS points or **image processing** to identify defects in manufacturing. | **Intrusion detection in networks** or **detecting abnormal gene expressions**, where the definition of \"normal\" can vary across different regions of the data. |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 3: What are the key components of a Time Series? Explain each with one example.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "A time series is a sequence of data points collected over time. To better understand and model a time series, it is often decomposed into four key components:\n",
                "\n",
                "#### 1. Trend (T)\n",
                "The trend represents the long-term, underlying direction of the data. It shows whether the series is generally increasing, decreasing, or remaining constant over an extended period.\n",
                "- **Example:** The steady increase in global average temperatures recorded over the last century. \n",
                "\n",
                "#### 2. Seasonality (S)\n",
                "Seasonality refers to regular, predictable patterns or fluctuations that repeat over a fixed and known period of time. This period can be daily, weekly, monthly, or yearly.\n",
                "- **Example:** Retail sales of ice cream consistently peaking every summer and dropping every winter. \n",
                "\n",
                "#### 3. Cyclical Component (C)\n",
                "The cyclical component consists of patterns that are not of a fixed period, unlike seasonality. These fluctuations are often related to longer-term economic or business cycles and their duration is usually at least 2 years.\n",
                "- **Example:** The boom-and-bust cycles in the housing market, which may repeat every 5-10 years, influenced by broad economic conditions.\n",
                "\n",
                "#### 4. Irregular / Residual Component (I or R)\n",
                "This component, also known as noise or random variation, is what remains after the trend, seasonality, and cyclical components have been removed from the time series. It represents the unpredictable and unsystematic fluctuations in the data.\n",
                "- **Example:** A sudden, sharp drop in the stock price of a company due to unexpected negative news."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 4: Define Stationarity in time series. How can you test and transform a non-stationary series into a stationary one?\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "#### Definition of Stationarity\n",
                "A time series is said to be **stationary** if its statistical properties‚Äîspecifically its mean, variance, and autocorrelation‚Äîare all constant over time. This means that a stationary series does not have a trend or seasonal effects, and its statistical characteristics are independent of the time at which they are observed. This property is a fundamental assumption for many time series forecasting models like ARIMA.\n",
                "\n",
                "#### How to Test for Stationarity\n",
                "You can test for stationarity using both visual methods and statistical tests:\n",
                "1.  **Visual Inspection**: Plot the time series and look for obvious trends or seasonal patterns. Plotting the rolling mean and rolling standard deviation can also help; if they are not constant, the series is likely non-stationary.\n",
                "2.  **Statistical Tests**: The most common test is the **Augmented Dickey-Fuller (ADF) Test**.\n",
                "    - **Null Hypothesis ($H_0$)**: The time series is non-stationary (it has a unit root).\n",
                "    - **Alternative Hypothesis ($H_1$)**: The time series is stationary.\n",
                "    To accept that the series is stationary, we need to reject the null hypothesis. This is done by checking if the **p-value** from the test is less than a chosen significance level (e.g., 0.05).\n",
                "\n",
                "#### How to Transform a Non-Stationary Series into a Stationary One\n",
                "If a time series is found to be non-stationary, it must be transformed before applying forecasting models. Common techniques include:\n",
                "1.  **Differencing**: This is the most common method. It involves computing the difference between consecutive observations. First-order differencing is calculated as:\n",
                "    $$Y'_t = Y_t - Y_{t-1}$$\n",
                "    If the series is still not stationary, you can apply second-order differencing (i.e., difference the differenced series). This helps to remove trends.\n",
                "\n",
                "2.  **Transformation**: To stabilize a non-constant variance, you can apply mathematical transformations like taking the **logarithm**, **square root**, or **Box-Cox transformation** of the series.\n",
                "\n",
                "Often, a combination of transformation and differencing (e.g., taking the log and then differencing) is required to make a series fully stationary."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 5: Differentiate between AR, MA, ARIMA, SARIMA, and SARIMAX models in terms of structure and application.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "| Model     | Full Name                           | Structure & Key Idea                                                                                                     | Application / Use Case                                                                                      |\n",
                "| :-------- | :---------------------------------- | :----------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------- |\n",
                "| **AR** | AutoRegressive                      | `AR(p)`: The current value ($Y_t$) is a linear combination of its own `p` past values. **Depends on past values.** | Modeling stationary time series where the next value has a strong correlation with recent past values.    |\n",
                "| **MA** | Moving Average                      | `MA(q)`: The current value ($Y_t$) is a linear combination of the `q` past forecast errors. **Depends on past errors.** | Modeling stationary time series where shocks or random spikes affect the output for a fixed duration.     |\n",
                "| **ARIMA** | AutoRegressive Integrated Moving Average | `ARIMA(p,d,q)`: Combines AR and MA with `d` levels of **differencing (Integration)** to handle non-stationary data.         | Forecasting non-stationary data that has a clear trend but no seasonality, like stock prices or economic GDP. |\n",
                "| **SARIMA**| Seasonal ARIMA                      | `SARIMA(p,d,q)(P,D,Q)m`: Extends ARIMA by adding **seasonal components** (`P,D,Q`) where `m` is the length of the season (e.g., 12 for monthly data). | Forecasting data with both trend and clear seasonality, such as monthly airline passenger numbers or quarterly retail sales. |\n",
                "| **SARIMAX**| SARIMA with eXogenous Variables     | `SARIMAX`: Extends SARIMA to include **external predictor variables (exogenous variables)** that can influence the target series. | Forecasting electricity demand (`Y_t`) using weather forecasts (temperature, holidays) as external predictors. This often leads to more accurate models. |\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 6: Load a time series dataset (e.g., AirPassengers), plot the original series, and decompose it into trend, seasonality, and residual components.\n",
                "---\n",
                "**Answer:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from statsmodels.tsa.seasonal import seasonal_decompose\n",
                "from statsmodels.datasets import get_rdataset\n",
                "\n",
                "# 1. Load the AirPassengers dataset\n",
                "# The dataset is available in statsmodels, but we'll use R's dataset for easy access\n",
                "air_passengers = get_rdataset(\"AirPassengers\").data\n",
                "\n",
                "# Convert the 'time' column to a proper datetime index\n",
                "# R dataset time is in float format e.g., 1949.000\n",
                "air_passengers['time'] = pd.to_datetime(air_passengers['time'].apply(\n",
                "    lambda x: f\"{int(x)}-{int((x - int(x)) * 12) + 1:02d}\"\n",
                "))\n",
                "air_passengers.set_index('time', inplace=True)\n",
                "series = air_passengers['value']\n",
                "\n",
                "# 2. Plot the original series\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(series)\n",
                "plt.title('Original AirPassengers Time Series')\n",
                "plt.xlabel('Year')\n",
                "plt.ylabel('Number of Passengers')\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# 3. Decompose the time series\n",
                "# We use a multiplicative model for decomposition as the seasonality grows with the trend\n",
                "decomposition = seasonal_decompose(series, model='multiplicative')\n",
                "\n",
                "# 4. Plot the decomposed components\n",
                "fig = decomposition.plot()\n",
                "fig.set_size_inches(12, 8)\n",
                "plt.suptitle('Time Series Decomposition of AirPassengers Data', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Output Explanation:**\n",
                "\n",
                "The code first plots the original time series of airline passengers, which clearly shows an upward trend and a repeating yearly seasonal pattern.\n",
                "\n",
                "Next, it performs a seasonal decomposition and plots the four resulting components:\n",
                "1.  **Observed**: The original time series data.\n",
                "2.  **Trend**: A smooth line showing the long-term upward movement in the number of passengers.\n",
                "3.  **Seasonal**: A repeating wave pattern that captures the yearly fluctuations (peaks in summer, troughs in winter).\n",
                "4.  **Resid**: The random, irregular noise remaining in the data after removing the trend and seasonal components."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 7: Apply Isolation Forest on a numerical dataset (e.g., NYC Taxi Fare) to detect anomalies. Visualize the anomalies on a 2D scatter plot.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "For this example, we'll create a synthetic dataset that mimics NYC Taxi Fare data, with features for `trip_distance` and `fare_amount`. We will then introduce some anomalies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.ensemble import IsolationForest\n",
                "\n",
                "# 1. Generate synthetic taxi fare data\n",
                "np.random.seed(42)\n",
                "# Generate 500 normal trips\n",
                "n_samples = 500\n",
                "trip_distance = np.random.uniform(1, 15, n_samples)  # Miles\n",
                "fare_amount = 2.50 + (trip_distance * 2.75) + np.random.normal(0, 3, n_samples) # Base fare + per mile + noise\n",
                "normal_data = np.column_stack((trip_distance, fare_amount))\n",
                "\n",
                "# Generate 20 anomaly trips\n",
                "n_anomalies = 20\n",
                "anomaly_dist = np.random.uniform(0.1, 25, n_anomalies)\n",
                "anomaly_fare = np.random.uniform(5, 200, n_anomalies)\n",
                "anomalies = np.column_stack((anomaly_dist, anomaly_fare))\n",
                "\n",
                "# Combine normal data and anomalies\n",
                "X = np.vstack((normal_data, anomalies))\n",
                "\n",
                "# 2. Apply Isolation Forest\n",
                "# Set contamination to the known proportion of anomalies\n",
                "iso_forest = IsolationForest(contamination=float(n_anomalies) / (n_samples + n_anomalies), random_state=42)\n",
                "y_pred = iso_forest.fit_predict(X)\n",
                "\n",
                "# y_pred will be 1 for inliers and -1 for outliers (anomalies)\n",
                "outlier_mask = y_pred == -1\n",
                "\n",
                "# 3. Visualize the anomalies\n",
                "plt.figure(figsize=(10, 7))\n",
                "\n",
                "# Plot the inliers (normal data)\n",
                "plt.scatter(X[~outlier_mask, 0], X[~outlier_mask, 1], c='blue', label='Normal Trips', alpha=0.6)\n",
                "\n",
                "# Plot the outliers (anomalies)\n",
                "plt.scatter(X[outlier_mask, 0], X[outlier_mask, 1], c='red', marker='x', s=100, label='Anomalies')\n",
                "\n",
                "plt.title('Anomaly Detection in Taxi Fare Data using Isolation Forest')\n",
                "plt.xlabel('Trip Distance (miles)')\n",
                "plt.ylabel('Fare Amount ($)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Output Explanation:**\n",
                "\n",
                "The scatter plot visualizes the synthetic taxi trip data. Most data points (blue dots) follow a clear linear relationship: as trip distance increases, so does the fare. The Isolation Forest algorithm successfully identifies the points that deviate from this pattern, marking them as red 'x's. These anomalies represent trips that are either unusually expensive for a short distance or unusually cheap for a long distance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 8: Train a SARIMA model on the monthly airline passengers dataset. Forecast the next 12 months and visualize the results.\n",
                "---\n",
                "**Answer:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
                "from statsmodels.datasets import get_rdataset\n",
                "\n",
                "# 1. Load and prepare the data\n",
                "air_passengers = get_rdataset(\"AirPassengers\").data\n",
                "air_passengers['time'] = pd.to_datetime(air_passengers['time'].apply(\n",
                "    lambda x: f\"{int(x)}-{int((x - int(x)) * 12) + 1:02d}\"\n",
                "))\n",
                "air_passengers.set_index('time', inplace=True)\n",
                "series = air_passengers['value']\n",
                "\n",
                "# 2. Train a SARIMA model\n",
                "# A commonly used order for this dataset is (1,1,1)x(1,1,1,12)\n",
                "# (p,d,q) = (1,1,1) for non-seasonal components\n",
                "# (P,D,Q,m) = (1,1,1,12) for seasonal components with a monthly period (m=12)\n",
                "model = SARIMAX(series, \n",
                "                order=(1, 1, 1), \n",
                "                seasonal_order=(1, 1, 1, 12),\n",
                "                enforce_stationarity=False,\n",
                "                enforce_invertibility=False)\n",
                "\n",
                "results = model.fit(disp=False)\n",
                "\n",
                "# 3. Forecast the next 12 months\n",
                "forecast_steps = 12\n",
                "forecast = results.get_forecast(steps=forecast_steps)\n",
                "\n",
                "# Get forecast values and confidence intervals\n",
                "pred_values = forecast.predicted_mean\n",
                "pred_ci = forecast.conf_int()\n",
                "\n",
                "# 4. Visualize the results\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(series, label='Observed')\n",
                "plt.plot(pred_values, label='Forecast', color='red')\n",
                "plt.fill_between(pred_ci.index, \n",
                "                 pred_ci.iloc[:, 0], \n",
                "                 pred_ci.iloc[:, 1], color='pink', alpha=0.5, label='95% Confidence Interval')\n",
                "\n",
                "plt.title('Air Passengers Forecast using SARIMA')\n",
                "plt.xlabel('Year')\n",
                "plt.ylabel('Number of Passengers')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Output Explanation:**\n",
                "\n",
                "The plot shows the original Air Passengers data (blue line) up to the end of 1960. The SARIMA model's forecast for the next 12 months is shown as a red line, which continues the upward trend and seasonal pattern observed in the historical data. The pink shaded area represents the 95% confidence interval, indicating the range within which the true future values are likely to fall."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 9: Apply Local Outlier Factor (LOF) on any numerical dataset to detect anomalies and visualize them using matplotlib.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "We will generate a synthetic dataset with clusters of varying densities to demonstrate the strength of LOF."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.neighbors import LocalOutlierFactor\n",
                "from sklearn.datasets import make_blobs\n",
                "\n",
                "# 1. Generate synthetic data\n",
                "n_samples = 300\n",
                "blobs_params = dict(random_state=42, n_samples=n_samples, n_features=2)\n",
                "X, _ = make_blobs(centers=[[0, 0], [5, 5]], cluster_std=[0.5, 1.5], **blobs_params)\n",
                "\n",
                "# Add some random noise points (outliers)\n",
                "np.random.seed(42)\n",
                "outliers = np.random.uniform(low=-4, high=8, size=(20, 2))\n",
                "X = np.vstack([X, outliers])\n",
                "\n",
                "# 2. Apply Local Outlier Factor (LOF)\n",
                "lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
                "y_pred = lof.fit_predict(X)\n",
                "\n",
                "# LOF returns 1 for inliers and -1 for outliers\n",
                "outlier_mask = y_pred == -1\n",
                "\n",
                "# 3. Visualize the results\n",
                "plt.figure(figsize=(10, 7))\n",
                "\n",
                "# Plot the inliers\n",
                "plt.scatter(X[~outlier_mask, 0], X[~outlier_mask, 1], c='blue', label='Inliers', alpha=0.7)\n",
                "\n",
                "# Plot the outliers\n",
                "plt.scatter(X[outlier_mask, 0], X[outlier_mask, 1], c='red', marker='x', s=100, label='Outliers')\n",
                "\n",
                "plt.title('Anomaly Detection using Local Outlier Factor (LOF)')\n",
                "plt.xlabel('Feature 1')\n",
                "plt.ylabel('Feature 2')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Output Explanation:**\n",
                "\n",
                "The plot shows two main clusters of data (blue dots) with different densities. The Local Outlier Factor algorithm correctly identifies the scattered points (red 'x's) as outliers because their local density is much lower compared to their neighbors. This demonstrates LOF's ability to find anomalies even when the data has regions of varying density."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Question 10: You are working as a data scientist for a power grid monitoring company... Explain your real-time data science workflow.\n",
                "---\n",
                "**Answer:**\n",
                "\n",
                "As a data scientist for a power grid company, my goal is to build a robust system for real-time anomaly detection and short-term energy demand forecasting. Here is my proposed workflow:\n",
                "\n",
                "#### 1. Anomaly Detection in Streaming Data\n",
                "\n",
                "Given the real-time, high-frequency nature of the data (every 15 minutes), the choice of algorithm must prioritize speed and efficiency.\n",
                "\n",
                "* **Chosen Algorithm**: **Isolation Forest**.\n",
                "* **Why?**: Isolation Forest is computationally efficient and scales well with large datasets, making it ideal for streaming applications. Its approach of isolating anomalies rather than profiling normal data works well for real-time detection. While LOF is powerful, its computational complexity ($O(n^2)$) makes it unsuitable for high-velocity streams. DBSCAN would require continuous re-clustering and is sensitive to density parameters which might shift over time.\n",
                "* **Implementation**: I would train an initial Isolation Forest model on a historical dataset of normal energy consumption. For incoming data points, I would use the trained model to quickly calculate an anomaly score. If the score exceeds a predefined threshold, an alert would be triggered for investigation.\n",
                "\n",
                "#### 2. Time Series Model for Short-Term Forecasting\n",
                "\n",
                "The forecasting model needs to be sophisticated enough to handle multiple seasonalities and external factors.\n",
                "\n",
                "* **Chosen Model**: **SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous variables)**.\n",
                "* **Why?**: Energy consumption has multiple layers of seasonality (daily, weekly, yearly) and is heavily influenced by external factors. SARIMAX is the perfect fit:\n",
                "    * The **SARIMA** component can model the complex trend and seasonal patterns inherent in energy usage.\n",
                "    * The **X (eXogenous)** component allows me to incorporate crucial external predictors like **weather conditions** (temperature, humidity), **day of the week**, and **public holidays**, which significantly improves forecast accuracy.\n",
                "\n",
                "#### 3. Validation and Performance Monitoring\n",
                "\n",
                "A model's performance can degrade over time (concept drift), so continuous validation is essential.\n",
                "\n",
                "* **Initial Validation**: I would use a **time-series cross-validation** technique on historical data. A hold-out set (e.g., the most recent month of data) would be used to evaluate the final model's performance using metrics like **Mean Absolute Percentage Error (MAPE)** and **Root Mean Squared Error (RMSE)**.\n",
                "* **Ongoing Monitoring**: Once deployed, the system would continuously monitor the model's accuracy by comparing its forecasts against the actual energy usage. I would set up a dashboard to track MAPE over time. If the error rate consistently exceeds a certain threshold, an alert would be triggered to notify the data science team. The model would be **retrained periodically** (e.g., weekly or monthly) on fresh data to ensure it adapts to new patterns.\n",
                "\n",
                "#### 4. Business Impact and Operational Benefits\n",
                "\n",
                "This dual anomaly detection and forecasting solution would provide significant value:\n",
                "\n",
                "* **Operational Efficiency**: Accurate short-term forecasts allow grid operators to perform **load balancing** more effectively, ensuring a stable power supply and preventing costly blackouts. It helps in deciding when to bring additional power plants online or purchase energy from other grids.\n",
                "* **Cost Reduction**: By predicting demand, the company can optimize energy generation and procurement, purchasing power when prices are low and avoiding expensive last-minute acquisitions.\n",
                "* **Proactive Maintenance**: The real-time anomaly detection system can identify equipment malfunctions or potential energy theft (e.g., a sudden, unexplainable drop in consumption at a location), allowing for rapid response and minimizing losses.\n",
                "* **Strategic Planning**: Long-term analysis of the forecast data can help in planning for future infrastructure upgrades and investments."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}